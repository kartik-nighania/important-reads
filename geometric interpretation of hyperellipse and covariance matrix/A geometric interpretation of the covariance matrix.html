<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0078)http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/ -->
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" prefix="og: http://ogp.me/ns#"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="http://cdn.api.twitter.com/1/urls/count.json?callback=WPCOMSharing.update_twitter_count&amp;url=http%3A%2F%2Fwww.visiondummy.com%2F2014%2F04%2Fgeometric-interpretation-covariance-matrix%2F&amp;_=1497085825358"></script><link rel="stylesheet" type="text/css" href="./A geometric interpretation of the covariance matrix_files/default.include.933709.css" media="all"><script id="twitter-wjs" src="./A geometric interpretation of the covariance matrix_files/widgets.js"></script><script async="" src="./A geometric interpretation of the covariance matrix_files/analytics.js"></script><script type="text/javascript" src="./A geometric interpretation of the covariance matrix_files/default.include.9525a3.js"></script><link rel="profile" href="http://gmpg.org/xfn/11"><link rel="author" href="https://plus.google.com/+VincentSpruyt"><link rel="pingback" href="http://www.visiondummy.com/xmlrpc.php"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>A geometric interpretation of the covariance matrix</title><meta name="description" content="In this article, we provide a geometric interpretation of the covariance matrix, exploring the relation between linear transformations and data covariance."><meta name="keywords" content="covariance matrix, eigenvectors, eigendecomposition, PCA, linear transformation"><link rel="canonical" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="A geometric interpretation of the covariance matrix"><meta property="og:description" content="In this article, we provide a geometric interpretation of the covariance matrix, exploring the relation between linear transformations and data covariance."><meta property="og:url" content="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"><meta property="og:site_name" content="Computer vision for dummies"><meta property="article:tag" content="covariance matrix"><meta property="article:tag" content="eigendecomposition"><meta property="article:tag" content="Eigenvectors"><meta property="article:tag" content="linear transformation"><meta property="article:tag" content="PCA"><meta property="article:section" content="Linear algebra"><meta property="article:published_time" content="2014-04-24T12:09:38+00:00"><meta property="article:modified_time" content="2015-03-22T15:23:26+00:00"><meta property="og:updated_time" content="2015-03-22T15:23:26+00:00"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/04/covariances.png"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/03/gaussiandensity.png"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/04/transformeddata.png"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/04/eigenvectors.png"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/04/eigenvectors_covariance.png"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/04/whiteneddata.png"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/04/stretcheddata.png"><meta property="og:image" content="http://www.visiondummy.com/wp-content/uploads/2014/04/lineartrans.png"><link rel="alternate" type="application/rss+xml" title="Computer vision for dummies » Feed" href="http://www.visiondummy.com/feed/"><link rel="alternate" type="application/rss+xml" title="Computer vision for dummies » Comments Feed" href="http://www.visiondummy.com/comments/feed/"><link rel="alternate" type="application/rss+xml" title="Computer vision for dummies » A geometric interpretation of the covariance matrix Comments Feed" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/feed/"><link rel="stylesheet" id="optinforms-googleFont-css" href="./A geometric interpretation of the covariance matrix_files/css" type="text/css" media="all"><link rel="stylesheet" id="optinforms-stylesheet-css" href="./A geometric interpretation of the covariance matrix_files/optinforms.css" type="text/css" media="all"><link rel="stylesheet" id="toc-screen-css" href="./A geometric interpretation of the covariance matrix_files/screen.min.css" type="text/css" media="all"><link rel="stylesheet" id="seed-wnb-css-css" href="./A geometric interpretation of the covariance matrix_files/seed_wnb.css" type="text/css" media="all"><link rel="stylesheet" id="genericons-css" href="./A geometric interpretation of the covariance matrix_files/genericons.css" type="text/css" media="all"> <script type="text/javascript">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','__gaTracker');__gaTracker('create','UA-46821096-2','auto');__gaTracker('set','forceSSL',true);__gaTracker('send','pageview');</script><script type="text/javascript" src="./A geometric interpretation of the covariance matrix_files/placeholder.js"></script><script type="text/javascript">var seed_wnb_js_localize={"msg":"Check out my top-4 of must-read","button_link":"http:\/\/www.visiondummy.com\/machine-learning-books\/","button_label":"machine learning books","button_target":"_blank"};</script><script type="text/javascript" src="./A geometric interpretation of the covariance matrix_files/seed_wnb.js"></script><link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.visiondummy.com/xmlrpc.php?rsd"><link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.visiondummy.com/wp-includes/wlwmanifest.xml"><meta name="generator" content="WordPress 3.8.21"><link rel="shortlink" href="http://www.visiondummy.com/?p=440"><script type="text/javascript">var ajaxurl="http://www.visiondummy.com/wp-admin/admin-ajax.php"</script><script type="text/javascript">/*<![CDATA[*/jQuery(document).ready(function(e){function i(t){args=l(e(t).html());var n=new Object;n.checking=false;n.previousY=-1;n.open_trigger=f(args,"open-trigger","");n.close_trigger=f(args,"close-trigger","");n.timed_popup_delay=f(args,"timed-popup-delay",-1);n.enable_exit_intent=f(args,"enable-exit-intent-popup",false);n.enable_scroll=f(args,"enable-scroll-popup",false);n.scroll_percent=f(args,"scroll-percent",-1);n.scroll_trigger=f(args,"scroll-trigger","");n.popup_selector=f(args,"popup-selector","");n.popup_class=f(args,"popup-class","");n.cookie_duration=f(args,"cookie-duration",0);n.cookie_name=f(args,"cookie-name","");n.priority=f(args,"priority",0);return n}function s(e){if(false===r||r<e.priority){p(e);d(e.cookie_name,e.cookie_duration)}}function o(e){if(e.checking&&(false===r||r<e.priority)){p(e);d(e.cookie_name,e.cookie_duration)}e.checking=false}function u(t){if(false===r||r<t.priority){var n=e(window).scrollTop()+e(window).height(),i=e(document).height();if(t.scroll_percent>=0){if(n>i*t.scroll_percent/100){p(t)}}if(t.scroll_trigger){e(t.scroll_trigger).each(function(i,s){if(false===r||r<t.priority){var o=e(s).offset().top;if(o<n){p(t)}}})}}}function a(){var n=[];e(t).each(function(t){var r=i(this),o=v(r.cookie_name),a=e.isFunction(e.fn.on);if(o===null){if(r.timed_popup_delay>=0){setTimeout(function(){s(r)},r.timed_popup_delay*1e3)}if(r.enable_exit_intent){e(document).mousemove(function(e){c(e,r)})}if(r.enable_scroll&&(r.scroll_percent>=0||r.scroll_trigger)){if(a){e(window).on("scroll",function(e){u(r)})}else{e(window).bind("scroll",function(e){u(r)})}}}if(r.open_trigger){if(a){e("html").on("touchend click",r.open_trigger,function(e){if(e.target!==this)return;p(r);return false})}else{e(r.open_trigger).bind("touchend click",function(e){if(e.target!==this)return;p(r);return false})}}if(r.close_trigger){if(a){e("html").on("touchend click",r.close_trigger,function(e){if(e.target!==this)return;h(r);return false})}else{e(r.close_trigger).bind("touchend click",function(e){if(e.target!==this)return;h(r);return false})}}n.push(r)});e(document).keyup(function(e){if(e.keyCode!=27)return;for(var t=0;t<n.length;t++){h(n[t])}return false})}function f(e,t,n){var r=null;if(t in e){r=e[t];if(typeof n=="boolean"){return r=="true"}if(typeof n=="number"){return parseInt(r)}return r}return n}function l(t){var n=e.parseJSON(t),r=null;for(r in n){if("string"===typeof n[r]){n[r]=n[r].replace(/&quot;/g,'"');n[r]=n[r].replace(/&#039;/g,"'");n[r]=n[r].replace(/&lt;/g,"<");n[r]=n[r].replace(/&gt;/g,">");n[r]=n[r].replace(/&/g,"&")}}return n}function c(e,t){if(t.hasOpened)return;t.checking=false;if(e.clientY<t.previousY){var n=e.clientY+(e.clientY-t.previousY);if(n<=10){t.checking=true;setTimeout(function(){o(t)},1)}}t.previousY=e.clientY}function h(t){n=false;if(t.popup_selector&&t.popup_class){e(t.popup_selector).removeClass(t.popup_class)}}function p(t){r=t.priority;if(n)return;n=true;if(t.popup_selector&&t.popup_class){e(t.popup_selector).addClass(t.popup_class)}}function d(e,t){var n="",r=null;if(e){if(t!==0){r=new Date;r.setTime(r.getTime()+t*24*60*60*1e3);n="; expires="+r.toGMTString()}document.cookie=e+"=disable"+n+"; path=/"}}function v(e){if(e){var t=e+"=",n=document.cookie,r=n.indexOf(t);if(r>=0){r+=t.length;endIndex=n.indexOf(";",r);if(endIndex<0)endIndex=n.length;return n.substring(r,endIndex)}}return null}var t=".popupally-configuration",n=false,r=false;a()})/*]]>*/</script><style type="text/css">#popup-box-sxzw-1 .clear-sxzw, #popup-embedded-box-sxzw-1 .clear-sxzw{clear:both;height:0px;width:100%}#popup-box-sxzw-1.popupally-overlay-sxzw-1{width:100%;height:100%;overflow:hidden;position:fixed;bottom:0;right:0;display:none;background-color:rgba(80,80,80,0.5)}#popup-box-sxzw-1.popupally-opened-sxzw-1{display:block !important;z-index:9999}#popup-box-sxzw-1 .popupally-outer-sxzw-1{position:absolute;top:20%;left:50%;width:650px;background-color:#fefefe;margin-left:-325px;-webkit-box-shadow:0 10px 25px rgba(0,0,0,0.5);-moz-box-shadow:0 10px 25px rgba(0,0,0,0.5);box-shadow:0 10px 25px rgba(0,0,0,0.5);-webkit-border-radius:4px;-moz-border-radius:4px;border-radius:4px}#popup-embedded-box-sxzw-1.popupally-outer-embedded-sxzw-1{width:650px;background-color:#fefefe;margin:0 auto;clear:both}#popup-box-sxzw-1 .popupally-inner-sxzw-1, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1{overflow:visible;height:auto;padding:20px;text-align:center}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .content-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .content-sxzw{text-align:center;vertical-align:middle;width:100%;box-sizing:border-box;-moz-box-sizing:border-box}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .desc-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .desc-sxzw{font-family:"Palatino Linotype","Book Antiqua",Palatino,serif;font-size:28px;line-height:30px;font-weight:bold;margin:0 0 10px 0;color:#444;display:block;box-sizing:border-box;-moz-box-sizing:border-box;cursor:pointer}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="text"].field-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="text"].field-sxzw{display:block;padding:15px 12px;margin:0 auto 10px auto;width:100%;font-family:Arial,Helvetica,sans-serif;font-size:16px;line-height:21px;border:1px solid #d4d3d3;background:#f6f6f6;-webkit-box-shadow:0 1px 0 rgba(255,255,255,0.67),inset 0 1px 3px rgba(5,5,5,0.13);-moz-box-shadow:0 1px 0 rgba(255,255,255,0.67),inset 0 1px 3px rgba(5,5,5,0.13);box-shadow:0 1px 0 rgba(255,255,255,0.67),inset 0 1px 3px rgba(5,5,5,0.13);-webkit-border-radius:3px;-moz-border-radius:3px;border-radius:3px;box-sizing:border-box;-moz-box-sizing:border-box;height:auto}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw{font-family:Arial,Helvetica,sans-serif;font-size:22px;line-height:27px;background-color:#fd4326;border-style:solid;border-width:1pt;border-color:#C0C0C0;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;color:#fff;text-shadow:0 0.1em 0.3em rgba(0,0,0,0.3);cursor:pointer;-webkit-border-radius:3pt;-moz-border-radius:3pt;border-radius:3pt;width:100%;padding:15px 0;margin:10px auto;box-sizing:border-box;-moz-box-sizing:border-box;height:auto;background-image:none}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw:hover, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw:hover{background-color:#fd4326;color:#fff;background-image:none}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .privacy-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .privacy-sxzw{font-family:"Palatino Linotype","Book Antiqua",Palatino,serif;font-size:14px;line-height:14px;color:#444;display:block;margin:10px 0;padding:0;box-sizing:border-box;-moz-box-sizing:border-box}#popup-box-sxzw-1 .popupally-outer-sxzw-1 .popupally-close-sxzw{background-image:url('http://www.visiondummy.com/wp-content/plugins/popupally/resource/frontend/img/fancy_close.png');background-size:cover;display:block;position:absolute;top:-12px;right:-12px;width:30px;height:30px;color:white;cursor:pointer}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-row-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-row-sxzw{margin:20px 0;display:block}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-img-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-img-sxzw{height:120px;margin:0 20px 0 0;float:left;display:block}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-text-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-text-sxzw{height:auto;width:100%;vertical-align:top;text-align:left;font-family:"Palatino Linotype","Book Antiqua",Palatino,serif;font-size:24px;line-height:28px;color:#444;margin:0;padding:0;box-sizing:border-box;-moz-box-sizing:border-box}@media (max-width: 768px){#popup-box-sxzw-1 .popupally-outer-sxzw-1{width:480px;margin-left:-240px}#popup-embedded-box-sxzw-1.popupally-outer-embedded-sxzw-1{width:480px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .desc-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .desc-sxzw{font-size:24px;line-height:26px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="text"].field-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="text"].field-sxzw{padding:10px 10px;font-size:12px;line-height:18px;margin-bottom:7px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw{font-size:18px;padding:10px 0;margin-top:8px;margin-bottom:8px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .privacy-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .privacy-sxzw{font-size:10px;line-height:10px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-row-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-row-sxzw{margin:15px 0}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-img-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-img-sxzw{height:80px;margin:0 15px 0 0}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-text-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-text-sxzw{font-size:20px;line-height:22px}}@media (max-width: 520px){#popup-box-sxzw-1 .popupally-outer-sxzw-1{width:300px;margin-left:-150px}#popup-embedded-box-sxzw-1.popupally-outer-embedded-sxzw-1{width:300px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .desc-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .desc-sxzw{font-size:18px;line-height:20px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="text"].field-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="text"].field-sxzw{padding:10px 6px;font-size:10px;line-height:14px;margin-bottom:5px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .popupally-center-sxzw .content-sxzw input[type="submit"].submit-sxzw{font-size:16px;padding:8px 0;margin-top:4px;margin-bottom:4px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .privacy-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .privacy-sxzw{font-size:8px;line-height:8px}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-row-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-row-sxzw{margin:10px 0}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-img-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-img-sxzw{height:60px;margin:0 10px 0 0}#popup-box-sxzw-1 .popupally-inner-sxzw-1 .logo-text-sxzw, #popup-embedded-box-sxzw-1 .popupally-inner-sxzw-1 .logo-text-sxzw{font-size:12px;line-height:14px}}</style><style type="text/css">#bg_color{color:black}.wnb-bar-button{color:white;text-shadow:0 -1px 0 rgba(0,0,0,0.25);background-color:black}#wnb-bar{color:black;position:fixed;background-color:#fef3e7;background-image:-moz-linear-gradient(top,#fef3e7,#fde2c5);background-image:-ms-linear-gradient(top,#fef3e7,#fde2c5);background-image:-webkit-gradient(linear,0 0,0 100%,from(#fef3e7),to(#fde2c5));background-image:-webkit-linear-gradient(top,#fef3e7,#fde2c5);background-image:-o-linear-gradient(top,#fef3e7,#fde2c5);background-image:linear-gradient(top,#fef3e7,#fde2c5);background-repeat:repeat-x;filter:progid:DXImageTransform.Microsoft.gradient(startColorstr='#fef3e7',endColorstr='#fde2c5',GradientType=0)}</style><style type="text/css">#content{max-width:960px}#header #logo{float:left;display:inline;margin:0;margin-top:17px;max-width:960px;max-height:90px;width:100%}#header #logo img{max-width:470px;margin-left:0;max-height:90px;padding-right:5%}.site-title{margin-top:20px}</style><style type="text/css">body{font-size:14px;font-style:normal;color:#555}body{background:#fff}h1.entry-title{font-size:28px;font-style:normal;color:#555}a{color:#2D89A7}#pagenavi span.current, #pagenavi a:hover{background-color:#333 !important}#pagenavi a, #pagenavi span{background-color:#FD4326 !important;color:#fff !important}#navigation .current_page_item, #navigation .secondary ul li.current-menu-item, #navigation .secondary >ul >li a:hover{color:#fff !important;background:#FD4326 !important}a:hover{color:#FD4326 !important}#top-menu-inner, #top-menu ul ul a{background:#2693BA !important}#top-menu .first ul li.current-menu-item, #top-menu .first >ul >li a:hover{color:#fff !important;background:##359BED !important}#navigation, #navigation ul ul a{background:#333}#navigation-inner{border-color:#FD4326}.readmore{background:#FD4326}#sidebar h4, #sidebar h4 a{background:;border-bottom-color:#FD4326 !important}#bottom-menu h4{background: }</style><link rel="publisher" href="https://plus.google.com/103720721989943799753"><link rel="shortcut icon" href="http://www.visiondummy.com/wp-content/uploads/2014/05/robot_favico.ico"><script type="text/javascript" charset="utf-8" async="" src="./A geometric interpretation of the covariance matrix_files/button.b731e6c82269f70f6fc53634e044a4c6.js"></script></head><body class="single single-post postid-440 single-format-standard" id="top" style="padding-top: 33px;"><div id="wnb-bar"><span>Check out my top-4 of must-read</span> <a href="http://www.visiondummy.com/machine-learning-books/" target="_blank"><span class="wnb-bar-button">machine learning books</span></a></div><div class="wrapper"><div id="header"><div id="header-inner" class="clearfix"><div id="logo"> <a href="http://www.visiondummy.com/"><img style="float: left;" src="./A geometric interpretation of the covariance matrix_files/robot2.png" height="" width="" alt=""></a><h1 class="site-title"> <a href="http://www.visiondummy.com/" title="Computer vision for dummies" rel="home">Computer vision for dummies</a></h1></div><ul class="spicesocialwidget" id="MENUID"><li class="googleplus"> <a rel="nofollow" href="https://plus.google.com/+vincentspruyt" target="_blank" title="googleplus"> </a></li><li class="linkedin"> <a rel="nofollow" href="http://be.linkedin.com/in/vincentspruyt" target="_blank" title="linkedin"> </a></li><li class="youtube"> <a rel="nofollow" href="http://www.youtube.com/esurior" target="_blank" title="youtube"> </a></li></ul></div></div><div id="navigation" class="nav"><div id="navigation-inner" class="clearfix"><div class="secondary"><ul id="menu-menu-1" class="menu"><li class="home-icon"><a href="http://www.visiondummy.com/" title="Home"><img src="./A geometric interpretation of the covariance matrix_files/home.jpg" width="26" height="24" alt="Home"></a></li><li id="menu-item-17" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-17"><a href="http://www.visiondummy.com/aboutme/">About me</a></li><li id="menu-item-645" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-645"><a href="http://www.visiondummy.com/contact/">Contact</a></li><li id="menu-item-644" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-644"><a href="http://www.visiondummy.com/machine-learning-books/">Machine Learning Books: A review</a></li></ul></div></div><select><option value="">Go to...</option><option value="http://www.visiondummy.com/">Home</option><option value="http://www.visiondummy.com/aboutme/">About me</option><option value="http://www.visiondummy.com/contact/">Contact</option><option value="http://www.visiondummy.com/machine-learning-books/">Machine Learning Books: A review</option></select></div><div id="page"><div id="page-inner" class="clearfix"><div id="banner-top"></div><div id="content"><div id="crumbs"><a href="http://www.visiondummy.com/">Home</a> » <a href="http://www.visiondummy.com/category/math-basics/" title="View all posts in Math basics">Math basics</a> » <a href="http://www.visiondummy.com/category/math-basics/linear-algebra/" title="View all posts in Linear algebra">Linear algebra</a> » <span class="current">A geometric interpretation of the covariance matrix</span></div><div id="post-440" class="post clearfix"><h1 class="entry-title">A geometric interpretation of the covariance matrix</h1><div class="entry clearfix"><div id="toc_container" class="toc_light_blue no_bullets"><p class="toc_title">Contents <span class="toc_toggle">[<a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#">hide</a>]</span> <span class="toc_toggle">[<a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#">hide</a>]</span></p><ul class="toc_list"><li><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#Introduction"><span class="toc_number toc_depth_1">1</span> Introduction</a></li><li><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#Eigendecomposition_of_a_covariance_matrix"><span class="toc_number toc_depth_1">2</span> Eigendecomposition of a covariance matrix</a></li><li><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#Covariance_matrix_as_a_linear_transformation"><span class="toc_number toc_depth_1">3</span> Covariance matrix as a linear transformation</a></li><li><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#Conclusion"><span class="toc_number toc_depth_1">4</span> Conclusion</a></li></ul></div><h2><span id="Introduction">Introduction</span></h2><p>In this article, we provide an intuitive, geometric interpretation of the covariance matrix, by exploring the relation between linear transformations and the resulting data covariance. Most textbooks explain the shape of data based on the concept of covariance matrices. Instead, we take a backwards approach and explain the concept of covariance matrices based on the shape of data.</p><p>In a previous article, we discussed the concept of <a title="Why divide the sample variance by N-1?" href="http://www.visiondummy.com/2014/03/divide-variance-n-1/" target="_blank">variance</a>, and provided a derivation and proof of the well known formula to estimate the sample variance. Figure 1 was used in this article to show that the standard deviation, as the square root of the variance, provides a measure of how much the data is spread across the feature space.</p><div id="attachment_213" style="width: 524px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/gaussiandensity.png"><img class="size-full wp-image-213 " style="margin: 0px;" title="Normal distribution" alt="Normal distribution" src="./A geometric interpretation of the covariance matrix_files/gaussiandensity.png" width="514" height="396"></a><p class="wp-caption-text"><b>Figure 1.</b> Gaussian density function. For normally distributed data, 68% of the samples fall within the interval defined by the mean plus and minus the standard deviation.</p></div><p>We showed that an unbiased estimator of the sample variance can be obtained by:</p><p class="ql-center-displayed-equation" style="line-height: 129px;"><span class="ql-right-eqno"> (1) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-8511602375b6c3ba0dcf673f5fcdd8f9_l3.png" height="129" width="267" class="ql-img-displayed-equation " alt="\begin{align*} \sigma_x^2 &amp;= \frac{1}{N-1} \sum_{i=1}^N (x_i - \mu)^2\\ &amp;= \mathbb{E}[ (x - \mathbb{E}(x)) (x - \mathbb{E}(x))]\\ &amp;= \sigma(x,x) \end{align*}" title="Rendered by QuickLaTeX.com"></p><p>However, variance can only be used to explain the spread of the data in the directions parallel to the axes of the feature space. Consider the 2D feature space shown by figure 2:</p><div id="attachment_390" style="width: 391px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/transformeddata.png"><img class="size-full wp-image-390   " style="margin: 0px;" title="Data with a positive covariance" alt="Data with a positive covariance" src="./A geometric interpretation of the covariance matrix_files/transformeddata.png" width="381" height="369"></a><p class="wp-caption-text"><b>Figure 2.</b> The diagnoal spread of the data is captured by the covariance.</p></div><p>For this data, we could calculate the variance <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-306b80c2caf6e1ce873db826824bae77_l3.png" class="ql-img-inline-formula " alt="\sigma(x,x)" title="Rendered by QuickLaTeX.com" height="23" width="61" style="vertical-align: -6px;"> in the x-direction and the variance <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-d0a6f8d59fd3d651e6d12aacb3804cb5_l3.png" class="ql-img-inline-formula " alt="\sigma(y,y)" title="Rendered by QuickLaTeX.com" height="23" width="59" style="vertical-align: -6px;"> in the y-direction. However, the horizontal spread and the vertical spread of the data does not explain the clear diagonal correlation. Figure 2 clearly shows that on average, if the x-value of a data point increases, then also the y-value increases, resulting in a positive correlation. This correlation can be captured by extending the notion of variance to what is called the ‘covariance’ of the data:</p><p class="ql-center-displayed-equation" style="line-height: 23px;"><span class="ql-right-eqno"> (2) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-476cbf37a8d4f3765fe0b2b58e5c8706_l3.png" height="23" width="304" class="ql-img-displayed-equation " alt="\begin{equation*} \sigma(x,y) = \mathbb{E}[ (x - \mathbb{E}(x)) (y - \mathbb{E}(y))] \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>For 2D data, we thus obtain <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-306b80c2caf6e1ce873db826824bae77_l3.png" class="ql-img-inline-formula " alt="\sigma(x,x)" title="Rendered by QuickLaTeX.com" height="23" width="61" style="vertical-align: -6px;">, <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-d0a6f8d59fd3d651e6d12aacb3804cb5_l3.png" class="ql-img-inline-formula " alt="\sigma(y,y)" title="Rendered by QuickLaTeX.com" height="23" width="59" style="vertical-align: -6px;">, <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-88d33eb20eafcc741815d0fffe208e01_l3.png" class="ql-img-inline-formula " alt="\sigma(x,y)" title="Rendered by QuickLaTeX.com" height="23" width="60" style="vertical-align: -6px;"> and <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-42efe14d58befabbf2f821c96ced0b4a_l3.png" class="ql-img-inline-formula " alt="\sigma(y,x)" title="Rendered by QuickLaTeX.com" height="23" width="60" style="vertical-align: -6px;">. These four values can be summarized in a matrix, called the covariance matrix:</p><p class="ql-center-displayed-equation" style="line-height: 64px;"><span class="ql-right-eqno"> (3) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-c3b2c0560068487dd51917cd55636781_l3.png" height="64" width="205" class="ql-img-displayed-equation " alt="\begin{equation*} \Sigma = \begin{bmatrix} \sigma(x,x) &amp; \sigma(x,y) \\[0.3em] \sigma(y,x) &amp; \sigma(y,y) \\[0.3em] \end{bmatrix} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>If x is positively correlated with y, y is also positively correlated with x. In other words, we can state that <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-a9f6d2d1f35bd9860e5975cd6a893877_l3.png" class="ql-img-inline-formula " alt="\sigma(x,y) = \sigma(y,x)" title="Rendered by QuickLaTeX.com" height="23" width="150" style="vertical-align: -6px;">. Therefore, the covariance matrix is always a symmetric matrix with the variances on its diagonal and the covariances off-diagonal. Two-dimensional normally distributed data is explained completely by its mean and its <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-9550d59c0c85b85636acad265530a8ee_l3.png" class="ql-img-inline-formula " alt="2\times 2" title="Rendered by QuickLaTeX.com" height="15" width="45" style="vertical-align: 0px;"> covariance matrix. Similarly, a <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-9f4e76f38736d8026154c7113a886bc0_l3.png" class="ql-img-inline-formula " alt="3 \times 3" title="Rendered by QuickLaTeX.com" height="15" width="46" style="vertical-align: 0px;"> covariance matrix is used to capture the spread of three-dimensional data, and a <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-27211e8b64d0af6bb1c7c805a18af057_l3.png" class="ql-img-inline-formula " alt="N \times N" title="Rendered by QuickLaTeX.com" height="14" width="64" style="vertical-align: 0px;"> covariance matrix captures the spread of N-dimensional data.</p><p>Figure 3 illustrates how the overall shape of the data defines the covariance matrix:</p><div id="attachment_446" style="width: 503px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/covariances.png"><img class="size-full wp-image-446" style="margin: 0px;" title="The spread of the data is defined by its covariance matrix" alt="The spread of the data is defined by its covariance matrix" src="./A geometric interpretation of the covariance matrix_files/covariances.png" width="493" height="479"></a><p class="wp-caption-text"><b>Figure 3.</b> The covariance matrix defines the shape of the data. Diagonal spread is captured by the covariance, while axis-aligned spread is captured by the variance.</p></div><h2><span id="Eigendecomposition_of_a_covariance_matrix">Eigendecomposition of a covariance matrix</span></h2><p>In the next section, we will discuss how the covariance matrix can be interpreted as a linear operator that transforms white data into the data we observed. However, before diving into the technical details, it is important to gain an intuitive understanding of how eigenvectors and eigenvalues uniquely define the covariance matrix, and therefore the shape of our data.</p><p>As we saw in figure 3, the covariance matrix defines both the spread (variance), and the orientation (covariance) of our data. So, if we would like to represent the covariance matrix with a vector and its magnitude, we should simply try to find the vector that points into the direction of the largest spread of the data, and whose magnitude equals the spread (variance) in this direction.</p><p>If we define this vector as <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5663d3adf90e26dd70e1f371e6cd6eba_l3.png" class="ql-img-inline-formula " alt="\vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;">, then the projection of our data <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-6fe012cfdbc6f342dbd886ff568ed4ab_l3.png" class="ql-img-inline-formula " alt="D" title="Rendered by QuickLaTeX.com" height="14" width="17" style="vertical-align: 0px;"> onto this vector is obtained as <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-ccf1bdb39d78be778899729ac16806ba_l3.png" class="ql-img-inline-formula " alt="\vec{v}^{\intercal} D" title="Rendered by QuickLaTeX.com" height="15" width="37" style="vertical-align: 0px;">, and the variance of the projected data is <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-e76086e0b82464aff045e27892d04123_l3.png" class="ql-img-inline-formula " alt="\vec{v}^{\intercal} \Sigma \vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="49" style="vertical-align: 0px;">. Since we are looking for the vector <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5663d3adf90e26dd70e1f371e6cd6eba_l3.png" class="ql-img-inline-formula " alt="\vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;"> that points into the direction of the largest variance, we should choose its components such that the covariance matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-e76086e0b82464aff045e27892d04123_l3.png" class="ql-img-inline-formula " alt="\vec{v}^{\intercal} \Sigma \vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="49" style="vertical-align: 0px;"> of the projected data is as large as possible. Maximizing any function of the form <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-e76086e0b82464aff045e27892d04123_l3.png" class="ql-img-inline-formula " alt="\vec{v}^{\intercal} \Sigma \vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="49" style="vertical-align: 0px;"> with respect to <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5663d3adf90e26dd70e1f371e6cd6eba_l3.png" class="ql-img-inline-formula " alt="\vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;">, where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5663d3adf90e26dd70e1f371e6cd6eba_l3.png" class="ql-img-inline-formula " alt="\vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;"> is a normalized unit vector, can be formulated as a so called <a href="http://en.wikipedia.org/wiki/Rayleigh_quotient" onclick="__gaTracker(&#39;send&#39;, &#39;event&#39;, &#39;outbound-article&#39;, &#39;http://en.wikipedia.org/wiki/Rayleigh_quotient&#39;, &#39;Rayleigh Quotient&#39;);" title="Rayleigh Quotient" target="_blank">Rayleigh Quotient</a>. The maximum of such a Rayleigh Quotient is obtained by setting <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5663d3adf90e26dd70e1f371e6cd6eba_l3.png" class="ql-img-inline-formula " alt="\vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;"> equal to the largest eigenvector of matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-66f091b3d894ca4b0418d9487b6b7e8a_l3.png" class="ql-img-inline-formula " alt="\Sigma" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;">.</p><p>In other words, the largest eigenvector of the covariance matrix always points into the direction of the largest variance of the data, and the magnitude of this vector equals the corresponding eigenvalue. The second largest eigenvector is always orthogonal to the largest eigenvector, and points into the direction of the second largest spread of the data.</p><p>Now let’s have a look at some examples. In an earlier article we saw that a linear transformation matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-99bdf2edc1f86c3fa1d60f4d82513c7d_l3.png" class="ql-img-inline-formula " alt="T" title="Rendered by QuickLaTeX.com" height="14" width="15" style="vertical-align: 0px;"> is completely defined by its <a title="What are eigenvectors and eigenvalues?" href="http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/" target="_blank">eigenvectors and eigenvalues</a>. Applied to the covariance matrix, this means that:<br> <a name="id3483335494"></a></p><p class="ql-center-displayed-equation" style="line-height: 15px;"><span class="ql-right-eqno"> (4) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-a17919125852783f2014314d7368316e_l3.png" height="15" width="79" class="ql-img-displayed-equation " alt="\begin{equation*}  \Sigma \vec{v} = \lambda \vec{v} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5663d3adf90e26dd70e1f371e6cd6eba_l3.png" class="ql-img-inline-formula " alt="\vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;"> is an eigenvector of <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-66f091b3d894ca4b0418d9487b6b7e8a_l3.png" class="ql-img-inline-formula " alt="\Sigma" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;">, and <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-50bc2c4701f0a0dd472fdd7dad5c47d9_l3.png" class="ql-img-inline-formula " alt="\lambda" title="Rendered by QuickLaTeX.com" height="14" width="11" style="vertical-align: 0px;"> is the corresponding eigenvalue.</p><p>If the covariance matrix of our data is a diagonal matrix, such that the covariances are zero, then this means that the variances must be equal to the eigenvalues <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-50bc2c4701f0a0dd472fdd7dad5c47d9_l3.png" class="ql-img-inline-formula " alt="\lambda" title="Rendered by QuickLaTeX.com" height="14" width="11" style="vertical-align: 0px;">. This is illustrated by figure 4, where the eigenvectors are shown in green and magenta, and where the eigenvalues clearly equal the variance components of the covariance matrix.</p><div id="attachment_603" style="width: 810px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/eigenvectors.png"><img src="./A geometric interpretation of the covariance matrix_files/eigenvectors.png" alt="Eigenvectors of a covariance matrix" width="800" height="383" class="size-full wp-image-603"></a><p class="wp-caption-text"><b>Figure 4.</b> Eigenvectors of a covariance matrix</p></div><p>However, if the covariance matrix is not diagonal, such that the covariances are not zero, then the situation is a little more complicated. The eigenvalues still represent the variance magnitude in the direction of the largest spread of the data, and the variance components of the covariance matrix still represent the variance magnitude in the direction of the x-axis and y-axis. But since the data is not axis aligned, these values are not the same anymore as shown by figure 5.</p><div id="attachment_604" style="width: 810px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/eigenvectors_covariance.png"><img src="./A geometric interpretation of the covariance matrix_files/eigenvectors_covariance.png" alt="Eigenvectors with covariance" width="800" height="382" class="size-full wp-image-604"></a><p class="wp-caption-text"><b>Figure 5.</b> Eigenvalues versus variance</p></div><p>By comparing figure 5 with figure 4, it becomes clear that the eigenvalues represent the variance of the data along the eigenvector directions, whereas the variance components of the covariance matrix represent the spread along the axes. If there are no covariances, then both values are equal.</p><h2><span id="Covariance_matrix_as_a_linear_transformation">Covariance matrix as a linear transformation</span></h2><p>Now let’s forget about covariance matrices for a moment. Each of the examples in figure 3 can simply be considered to be a linearly transformed instance of figure 6:</p><div id="attachment_447" style="width: 391px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/whiteneddata.png"><img class="size-full wp-image-447" style="margin: 0px;" title="White data" alt="White data" src="./A geometric interpretation of the covariance matrix_files/whiteneddata.png" width="381" height="369"></a><p class="wp-caption-text"><b>Figure 6.</b> Data with unit covariance matrix is called white data.</p></div><p>Let the data shown by figure 6 be <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-6fe012cfdbc6f342dbd886ff568ed4ab_l3.png" class="ql-img-inline-formula " alt="D" title="Rendered by QuickLaTeX.com" height="14" width="17" style="vertical-align: 0px;">, then each of the examples shown by figure 3 can be obtained by linearly transforming <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-6fe012cfdbc6f342dbd886ff568ed4ab_l3.png" class="ql-img-inline-formula " alt="D" title="Rendered by QuickLaTeX.com" height="14" width="17" style="vertical-align: 0px;">:</p><p class="ql-center-displayed-equation" style="line-height: 18px;"><span class="ql-right-eqno"> (5) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-7aecb171a514b3c704f078ec86182805_l3.png" height="18" width="87" class="ql-img-displayed-equation " alt="\begin{equation*} D&#39; = T \, D \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-99bdf2edc1f86c3fa1d60f4d82513c7d_l3.png" class="ql-img-inline-formula " alt="T" title="Rendered by QuickLaTeX.com" height="14" width="15" style="vertical-align: 0px;"> is a transformation matrix consisting of a rotation matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-026035461a80f8e10b18e494d1116782_l3.png" class="ql-img-inline-formula " alt="R" title="Rendered by QuickLaTeX.com" height="14" width="16" style="vertical-align: 0px;"> and a scaling matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-7f83dd23b1b356198dd90895630ebcef_l3.png" class="ql-img-inline-formula " alt="S" title="Rendered by QuickLaTeX.com" height="14" width="13" style="vertical-align: 0px;">:<br> <a name="id1585768567"></a></p><p class="ql-center-displayed-equation" style="line-height: 14px;"><span class="ql-right-eqno"> (6) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-2481ecd212935a8cc503131bf2596bf6_l3.png" height="14" width="81" class="ql-img-displayed-equation " alt="\begin{equation*} T = R \, S. \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>These matrices are defined as:</p><p class="ql-center-displayed-equation" style="line-height: 64px;"><span class="ql-right-eqno"> (7) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-78bf053271a867c2d5b7c2b30d3e7924_l3.png" height="64" width="211" class="ql-img-displayed-equation " alt="\begin{equation*} R = \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta) \\[0.3em] \sin(\theta) &amp; \cos(\theta) \end{bmatrix} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-a633c6dcc2aba17ef85b129e4fbcaf98_l3.png" class="ql-img-inline-formula " alt="\theta" title="Rendered by QuickLaTeX.com" height="14" width="10" style="vertical-align: 0px;"> is the rotation angle, and:</p><p class="ql-center-displayed-equation" style="line-height: 64px;"><span class="ql-right-eqno"> (8) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-0756bebe1440213107fea1005e1a655b_l3.png" height="64" width="120" class="ql-img-displayed-equation " alt="\begin{equation*} S = \begin{bmatrix} s_x &amp; 0 \\[0.3em] 0 &amp; s_y \end{bmatrix} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-197e94159cb0b049505c16b6448e224c_l3.png" class="ql-img-inline-formula " alt="s_x" title="Rendered by QuickLaTeX.com" height="12" width="18" style="vertical-align: -3px;"> and <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-1be2a4d3326735aa17afcfc4d6409278_l3.png" class="ql-img-inline-formula " alt="s_y" title="Rendered by QuickLaTeX.com" height="15" width="18" style="vertical-align: -6px;"> are the scaling factors in the x direction and the y direction respectively.</p><p>In the following paragraphs, we will discuss the relation between the covariance matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-66f091b3d894ca4b0418d9487b6b7e8a_l3.png" class="ql-img-inline-formula " alt="\Sigma" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;">, and the linear transformation matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-b40eb258e9e321e3d2262a5afffcc8bb_l3.png" class="ql-img-inline-formula " alt="T = R\, S" title="Rendered by QuickLaTeX.com" height="14" width="77" style="vertical-align: 0px;">.</p><p>Let’s start with unscaled (scale equals 1) and unrotated data. In statistics this is often refered to as ‘white data’ because its samples are drawn from a standard normal distribution and therefore correspond to white (uncorrelated) noise:</p><div id="attachment_394" style="width: 391px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/whiteneddata.png"><img class="size-full wp-image-394 " style="margin: 0px;" title="Whitened data" alt="Whitened data" src="./A geometric interpretation of the covariance matrix_files/whiteneddata.png" width="381" height="369"></a><p class="wp-caption-text"><b>Figure 7.</b> White data is data with a unit covariance matrix.</p></div><p>The covariance matrix of this ‘white’ data equals the identity matrix, such that the variances and standard deviations equal 1 and the covariance equals zero:</p><p class="ql-center-displayed-equation" style="line-height: 64px;"><span class="ql-right-eqno"> (9) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-22cfcbfd49a80711b48bee89d0ac5e9e_l3.png" height="64" width="218" class="ql-img-displayed-equation " alt="\begin{equation*} \Sigma = \begin{bmatrix} \sigma_x^2 &amp; 0 \\[0.3em] 0 &amp; \sigma_y^2 \\ \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\[0.3em] 0 &amp; 1 \\ \end{bmatrix} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>Now let’s scale the data in the x-direction with a factor 4:</p><p class="ql-center-displayed-equation" style="line-height: 64px;"><span class="ql-right-eqno"> (10) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-93925ded582a8e859f4efd17c75d7dc9_l3.png" height="64" width="141" class="ql-img-displayed-equation " alt="\begin{equation*} D&#39; = \begin{bmatrix} 4 &amp; 0 \\[0.3em] 0 &amp; 1 \\ \end{bmatrix} \, D \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>The data <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-69e87a5558d2fcd98b5a9d1292a4345e_l3.png" class="ql-img-inline-formula " alt="D&#39;" title="Rendered by QuickLaTeX.com" height="17" width="23" style="vertical-align: 0px;"> now looks as follows:</p><div id="attachment_400" style="width: 391px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/stretcheddata.png"><img class="size-full wp-image-400" style="margin: 0px;" title="Data with variance in the x-direction" alt="Data with variance in the x-direction" src="./A geometric interpretation of the covariance matrix_files/stretcheddata.png" width="381" height="369"></a><p class="wp-caption-text"><b>Figure 8.</b> Variance in the x-direction results in a horizontal scaling.</p></div><p>The covariance matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-6a91d339ba236a991b48b26135dd4246_l3.png" class="ql-img-inline-formula " alt="\Sigma&#39;" title="Rendered by QuickLaTeX.com" height="17" width="19" style="vertical-align: 0px;"> of <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-69e87a5558d2fcd98b5a9d1292a4345e_l3.png" class="ql-img-inline-formula " alt="D&#39;" title="Rendered by QuickLaTeX.com" height="17" width="23" style="vertical-align: 0px;"> is now:</p><p class="ql-center-displayed-equation" style="line-height: 64px;"><span class="ql-right-eqno"> (11) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-b17970f14e6400c5fc20c4b9c069abfd_l3.png" height="64" width="234" class="ql-img-displayed-equation " alt="\begin{equation*} \Sigma&#39; = \begin{bmatrix} \sigma_x^2 &amp; 0 \\[0.3em] 0 &amp; \sigma_y^2 \\ \end{bmatrix} = \begin{bmatrix} 16 &amp; 0 \\[0.3em] 0 &amp; 1 \\ \end{bmatrix} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>Thus, the covariance matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-6a91d339ba236a991b48b26135dd4246_l3.png" class="ql-img-inline-formula " alt="\Sigma&#39;" title="Rendered by QuickLaTeX.com" height="17" width="19" style="vertical-align: 0px;"> of the resulting data <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-69e87a5558d2fcd98b5a9d1292a4345e_l3.png" class="ql-img-inline-formula " alt="D&#39;" title="Rendered by QuickLaTeX.com" height="17" width="23" style="vertical-align: 0px;"> is related to the linear transformation <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-99bdf2edc1f86c3fa1d60f4d82513c7d_l3.png" class="ql-img-inline-formula " alt="T" title="Rendered by QuickLaTeX.com" height="14" width="15" style="vertical-align: 0px;"> that is applied to the original data as follows: <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-9bd559b313e798679ab85e7718dea765_l3.png" class="ql-img-inline-formula " alt="D&#39; = T \, D" title="Rendered by QuickLaTeX.com" height="17" width="87" style="vertical-align: 0px;">, where<br> <a name="id537686066"></a></p><p class="ql-center-displayed-equation" style="line-height: 64px;"><span class="ql-right-eqno"> (12) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-51df1544156ec5782e7799b4782b029b_l3.png" height="64" width="183" class="ql-img-displayed-equation " alt="\begin{equation*} T = \sqrt{\Sigma&#39;} = \begin{bmatrix} 4 &amp; 0 \\[0.3em] 0 &amp; 1 \\ \end{bmatrix}. \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>However, although equation (<a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#id537686066">12</a>) holds when the data is scaled in the x and y direction, the question rises if it also holds when a rotation is applied. To investigate the relation between the linear transformation matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-99bdf2edc1f86c3fa1d60f4d82513c7d_l3.png" class="ql-img-inline-formula " alt="T" title="Rendered by QuickLaTeX.com" height="14" width="15" style="vertical-align: 0px;"> and the covariance matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-6a91d339ba236a991b48b26135dd4246_l3.png" class="ql-img-inline-formula " alt="\Sigma&#39;" title="Rendered by QuickLaTeX.com" height="17" width="19" style="vertical-align: 0px;"> in the general case, we will therefore try to decompose the covariance matrix into the product of rotation and scaling matrices.</p><p>As we saw earlier, we can represent the covariance matrix by its eigenvectors and eigenvalues:<br> <a name="id3483335494"></a></p><p class="ql-center-displayed-equation" style="line-height: 15px;"><span class="ql-right-eqno"> (13) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-a17919125852783f2014314d7368316e_l3.png" height="15" width="79" class="ql-img-displayed-equation " alt="\begin{equation*}  \Sigma \vec{v} = \lambda \vec{v} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5663d3adf90e26dd70e1f371e6cd6eba_l3.png" class="ql-img-inline-formula " alt="\vec{v}" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;"> is an eigenvector of <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-66f091b3d894ca4b0418d9487b6b7e8a_l3.png" class="ql-img-inline-formula " alt="\Sigma" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;">, and <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-50bc2c4701f0a0dd472fdd7dad5c47d9_l3.png" class="ql-img-inline-formula " alt="\lambda" title="Rendered by QuickLaTeX.com" height="14" width="11" style="vertical-align: 0px;"> is the corresponding eigenvalue.</p><p>Equation (<a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#id3483335494">13</a>) holds for each eigenvector-eigenvalue pair of matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-66f091b3d894ca4b0418d9487b6b7e8a_l3.png" class="ql-img-inline-formula " alt="\Sigma" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;">. In the 2D case, we obtain two eigenvectors and two eigenvalues. The system of two equations defined by equation (<a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#id3483335494">13</a>) can be represented efficiently using matrix notation:<br> <a name="id1495159919"></a></p><p class="ql-center-displayed-equation" style="line-height: 15px;"><span class="ql-right-eqno"> (14) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-f6fdbf4f1af6863c9afc04f7418fdc6f_l3.png" height="15" width="97" class="ql-img-displayed-equation " alt="\begin{equation*}  \Sigma \, V = V \, L \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-1f1ceff6690e6ea05bc7802220277816_l3.png" class="ql-img-inline-formula " alt="V" title="Rendered by QuickLaTeX.com" height="14" width="16" style="vertical-align: 0px;"> is the matrix whose columns are the eigenvectors of <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-66f091b3d894ca4b0418d9487b6b7e8a_l3.png" class="ql-img-inline-formula " alt="\Sigma" title="Rendered by QuickLaTeX.com" height="15" width="13" style="vertical-align: 0px;"> and <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-f8016ff830b491e0b1f3122a41ccff3f_l3.png" class="ql-img-inline-formula " alt="L" title="Rendered by QuickLaTeX.com" height="14" width="14" style="vertical-align: 0px;"> is the diagonal matrix whose non-zero elements are the corresponding eigenvalues.</p><p>This means that we can represent the covariance matrix as a function of its eigenvectors and eigenvalues:<br> <a name="id2430180844"></a></p><p class="ql-center-displayed-equation" style="line-height: 20px;"><span class="ql-right-eqno"> (15) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-1bd7a6edabf351786ae510e2c02d1663_l3.png" height="20" width="117" class="ql-img-displayed-equation " alt="\begin{equation*}  \Sigma = V \, L \, V^{-1} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>Equation (<a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#id2430180844">15</a>) is called the eigendecomposition of the covariance matrix and can be obtained using a <a title="Singular Value Decomposition" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank">Singular Value Decomposition</a> algorithm. Whereas the eigenvectors represent the directions of the largest variance of the data, the eigenvalues represent the magnitude of this variance in those directions. In other words, <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-1f1ceff6690e6ea05bc7802220277816_l3.png" class="ql-img-inline-formula " alt="V" title="Rendered by QuickLaTeX.com" height="14" width="16" style="vertical-align: 0px;"> represents a rotation matrix, while <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-8235da3a0beb2b3fd48aef3af7ba37fa_l3.png" class="ql-img-inline-formula " alt="\sqrt{L}" title="Rendered by QuickLaTeX.com" height="22" width="32" style="vertical-align: -3px;"> represents a scaling matrix. The covariance matrix can thus be decomposed further as:<br> <a name="id2743526996"></a></p><p class="ql-center-displayed-equation" style="line-height: 20px;"><span class="ql-right-eqno"> (16) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-23df767e6fb3e95725feacf9467b019e_l3.png" height="20" width="133" class="ql-img-displayed-equation " alt="\begin{equation*}  \Sigma = R \, S \, S \, R^{-1} \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>where <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-7c0435dd691dcbce6e1b3121ba27bbd6_l3.png" class="ql-img-inline-formula " alt="R=V" title="Rendered by QuickLaTeX.com" height="14" width="61" style="vertical-align: 0px;"> is a rotation matrix and <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-3262ef230355c0fef4406d0637062a28_l3.png" class="ql-img-inline-formula " alt="S=\sqrt{L}" title="Rendered by QuickLaTeX.com" height="22" width="74" style="vertical-align: -3px;"> is a scaling matrix.</p><p>In equation (<a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#id1585768567">6</a>) we defined a linear transformation <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-35bf2c5b24b044c78d4ac3ecff5b2078_l3.png" class="ql-img-inline-formula " alt="T=R \, S" title="Rendered by QuickLaTeX.com" height="14" width="77" style="vertical-align: 0px;">. Since <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-7f83dd23b1b356198dd90895630ebcef_l3.png" class="ql-img-inline-formula " alt="S" title="Rendered by QuickLaTeX.com" height="14" width="13" style="vertical-align: 0px;"> is a diagonal scaling matrix, <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-e6305c54d65d420123b47b07e403a536_l3.png" class="ql-img-inline-formula " alt="S = S^{\intercal}" title="Rendered by QuickLaTeX.com" height="15" width="63" style="vertical-align: 0px;">. Furthermore, since <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-026035461a80f8e10b18e494d1116782_l3.png" class="ql-img-inline-formula " alt="R" title="Rendered by QuickLaTeX.com" height="14" width="16" style="vertical-align: 0px;"> is an orthogonal matrix, <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-945fcf37014dbf658e49128ad721040e_l3.png" class="ql-img-inline-formula " alt="R^{-1} = R^{\intercal}" title="Rendered by QuickLaTeX.com" height="19" width="90" style="vertical-align: 0px;">. Therefore, <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-205b466d7f5da5d6cff27d6859693391_l3.png" class="ql-img-inline-formula " alt="T^{\intercal} = (R \, S)^{\intercal} = S^{\intercal} \, R^{\intercal} = S \, R^{-1}" title="Rendered by QuickLaTeX.com" height="25" width="275" style="vertical-align: -6px;">. The covariance matrix can thus be written as:<br> <a name="id3282722977"></a></p><p class="ql-center-displayed-equation" style="line-height: 24px;"><span class="ql-right-eqno"> (17) </span><span class="ql-left-eqno"> &nbsp; </span><img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-5889c4c6b55d1c90107dd9fc09195d1c_l3.png" height="24" width="212" class="ql-img-displayed-equation " alt="\begin{equation*}  \Sigma = R \, S \, S \, R^{-1} = T \, T^{\intercal}, \end{equation*}" title="Rendered by QuickLaTeX.com"></p><p>In other words, if we apply the linear transformation defined by <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-35bf2c5b24b044c78d4ac3ecff5b2078_l3.png" class="ql-img-inline-formula " alt="T=R \, S" title="Rendered by QuickLaTeX.com" height="14" width="77" style="vertical-align: 0px;"> to the original white data <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-6fe012cfdbc6f342dbd886ff568ed4ab_l3.png" class="ql-img-inline-formula " alt="D" title="Rendered by QuickLaTeX.com" height="14" width="17" style="vertical-align: 0px;"> shown by figure 7, we obtain the rotated and scaled data <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-69e87a5558d2fcd98b5a9d1292a4345e_l3.png" class="ql-img-inline-formula " alt="D&#39;" title="Rendered by QuickLaTeX.com" height="17" width="23" style="vertical-align: 0px;"> with covariance matrix <img src="./A geometric interpretation of the covariance matrix_files/quicklatex.com-c518de6c373241128e4f4bbb640476a1_l3.png" class="ql-img-inline-formula " alt="T \, T^{\intercal} = \Sigma&#39; = R \, S \, S \, R^{-1}" title="Rendered by QuickLaTeX.com" height="19" width="211" style="vertical-align: 0px;">. This is illustrated by figure 10:</p><div id="attachment_407" style="width: 950px" class="wp-caption aligncenter"><a href="./A geometric interpretation of the covariance matrix_files/lineartrans.png"><img class="size-full wp-image-407 " style="margin: 0px;" title="The covariance matrix represents a linear transformation of the original data" alt="The covariance matrix represents a linear transformation of the original data" src="./A geometric interpretation of the covariance matrix_files/lineartrans.png" width="940" height="451"></a><p class="wp-caption-text"><b>Figure 10.</b> The covariance matrix represents a linear transformation of the original data.</p></div><p>The colored arrows in figure 10 represent the eigenvectors. The largest eigenvector, i.e. the eigenvector with the largest corresponding eigenvalue, always points in the direction of the largest variance of the data and thereby defines its orientation. Subsequent eigenvectors are always orthogonal to the largest eigenvector due to the orthogonality of rotation matrices.</p><h2><span id="Conclusion">Conclusion</span></h2><p>In this article we showed that the covariance matrix of observed data is directly related to a linear transformation of white, uncorrelated data. This linear transformation is completely defined by the eigenvectors and eigenvalues of the data. While the eigenvectors represent the rotation matrix, the eigenvalues correspond to the square of the scaling factor in each dimension.</p><p><strong>If you’re new to this blog, don’t forget to subscribe, or <a href="https://twitter.com/vincent_spruyt" onclick="__gaTracker(&#39;send&#39;, &#39;event&#39;, &#39;outbound-article&#39;, &#39;https://twitter.com/vincent_spruyt&#39;, &#39;follow me on twitter&#39;);" title="Follow me on Twitter!" target="_blank">follow me on twitter</a>!</strong><br></p><div id="optinforms-form5-container"><form method="post" target="_blank" action="http://visiondummy.us10.list-manage.com/subscribe/post?u=c435905e10ead915f3917d694&amp;id=bbdfb33a9f"><div id="optinforms-form5" style="background:#ffffff;"><div id="optinforms-form5-container-left"><div id="optinforms-form5-title" style="font-family:News Cycle; font-size:24px; line-height:24px; color:#fd4326">JOIN MY NEWSLETTER</div><input type="text" id="optinforms-form5-name-field" name="FNAME" placeholder="Enter Your Name" style="font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000000"><input type="text" id="optinforms-form5-email-field" name="EMAIL" placeholder="Enter Your Email" style="font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000000"><input type="submit" name="submit" id="optinforms-form5-button" value="SUBSCRIBE" style="font-family:Arial, Helvetica, sans-serif; font-size:16px; color:#FFFFFF; background-color:#fd4326"></div><div id="optinforms-form5-container-right"><div id="optinforms-form5-subtitle" style="font-family:Georgia; font-size:16px; color:#444444">Receive my newsletter to get notified when new articles and code snippets become available on my blog!</div><div id="optinforms-form5-disclaimer" style="font-family:Georgia, Times New Roman, Times, serif; font-size:14px; color:#727272">I hate spam. Your email address will not be sold or shared with anyone else.</div></div><div class="clear"></div></div><div class="clear"></div></form></div><div class="clear"></div><p></p><div id="snippet-box" style="background:#F5F5F5; color:#333333; border:1px solid #ACACAC;"><div class="snippet-title" style="background:#E4E4E4; color:#333333; border-bottom:1px solid #ACACAC;">Summary</div><div itemscope="" itemtype="http://schema.org/Article"><div class="snippet-image"><img width="180" itemprop="image" src="./A geometric interpretation of the covariance matrix_files/covariances.png"></div><div class="aio-info"><div class="snippet-label-img">Article Name</div><div class="snippet-data-img"><span itemprop="name">A geometric interpretation of the covariance matrix</span></div><div class="snippet-clear"></div><div class="snippet-label-img">Author</div><div class="snippet-data-img"><span itemprop="author">Vincent Spruyt</span></div><div class="snippet-clear"></div><div class="snippet-label-img">Description</div><div class="snippet-data-img"><span itemprop="description">In this article, we provide an intuitive, geometric interpretation of the covariance matrix, by exploring the relation between linear transformations and the resulting data covariance.</span></div><div class="snippet-clear"></div></div></div></div><div class="snippet-clear"></div><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this post with your social networks:</h3><div class="sd-content"><ul><li><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#" class="sharing-anchor sd-button share-more"><span>Share</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?share=facebook&amp;nb=1" target="_blank" title="Share on Facebook" id="sharing-facebook-440"><span>Facebook</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?share=google-plus-1&amp;nb=1" target="_blank" title="Click to share on Google+" id="sharing-google-440"><span>Google</span></a></li><li class="share-end"></li><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?share=twitter&amp;nb=1" target="_blank" title="Click to share on Twitter" id="sharing-twitter-440"><span>Twitter</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?share=linkedin&amp;nb=1" target="_blank" title="Click to share on LinkedIn" id="sharing-linkedin-440"><span>LinkedIn<span class="share-count">19</span></span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?share=reddit&amp;nb=1" target="_blank" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-end"></li></ul></div></div></div></div></div></div> <span class="postmeta_box"> <span class="auth"> <span itemprop="datePublished" class="timestamp updated">April 24, 2014</span> <span class="author vcard" itemprop="author" itemtype="http://schema.org/Person"><span class="fn">Vincent Spruyt</span></span></span><span itemprop="articleSection" class="postcateg"><a href="http://www.visiondummy.com/category/math-basics/linear-algebra/" title="View all posts in Linear algebra" rel="category tag">Linear algebra</a></span><span class="comp"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comments" title="Comment on A geometric interpretation of the covariance matrix">44 Comments</a></span> <span class="tags"><a href="http://www.visiondummy.com/tag/covariance-matrix/" rel="tag">covariance matrix</a>, <a href="http://www.visiondummy.com/tag/eigendecomposition/" rel="tag">eigendecomposition</a>, <a href="http://www.visiondummy.com/tag/eigenvectors/" rel="tag">Eigenvectors</a>, <a href="http://www.visiondummy.com/tag/linear-transformation/" rel="tag">linear transformation</a>, <a href="http://www.visiondummy.com/tag/pca/" rel="tag">PCA</a></span> </span><div class="gap"></div><div id="single-nav" class="clearfix"><div id="single-nav-left">«<strong><a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" rel="prev">The Curse of Dimensionality in classification</a></strong></div><div id="single-nav-right"><strong><a href="http://www.visiondummy.com/2014/05/feature-extraction-using-pca/" rel="next">Feature extraction using PCA</a></strong>»</div></div><div class="comments"><h3 id="Comments">comments</h3><div class="navigation"><div class="alignleft"></div><div class="alignright"></div></div><ol class="commentlist"><li class="comment even thread-even depth-1" id="comment-22"><div id="div-comment-22" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Chris</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-22"> May 14, 2014 at 3:42 pm</a></div><p>Great article thank you</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=22#respond" onclick="return addComment.moveForm(&quot;div-comment-22&quot;, &quot;22&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1 parent" id="comment-23"><div id="div-comment-23" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Alex</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-23"> May 14, 2014 at 8:09 pm</a></div><p>The covariance matrix is symmetric. Hence we can find a basis of orthonormal eigenvectors and then $\Sigma=VL V^T$.<br> From computational point of view it is much simpler to find $V^T$ than $V^{-1}$.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=23#respond" onclick="return addComment.moveForm(&quot;div-comment-23&quot;, &quot;23&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment byuser comment-author-esurior bypostauthor even depth-2" id="comment-26"><div id="div-comment-26" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://www.visiondummy.com/" rel="external nofollow" class="url">Vincent Spruyt</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-26"> May 15, 2014 at 7:59 am</a></div><p>Very true, Alex, and thanks for your comment! This is also written in the article: “Furthermore, since R is an orthogonal matrix, R^{-1} = R^T”. But you are right that I only mention this near the end of the article, mostly because it is easier to develop an intuitive understanding of the first part of the article by considering R^{-1} instead of R^T.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=26#respond" onclick="return addComment.moveForm(&quot;div-comment-26&quot;, &quot;26&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment odd alt thread-even depth-1 parent" id="comment-47"><div id="div-comment-47" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Brian</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-47"> May 24, 2014 at 1:03 am</a></div><p>Great post! I had a couple questions:<br> 1) The data D doesn’t need to be Gaussian does it?<br> 2) Is [9] reversed (should D be on the left)?</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=47#respond" onclick="return addComment.moveForm(&quot;div-comment-47&quot;, &quot;47&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment byuser comment-author-esurior bypostauthor even depth-2" id="comment-49"><div id="div-comment-49" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://www.visiondummy.com/" rel="external nofollow" class="url">Vincent Spruyt</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-49"> May 24, 2014 at 8:27 am</a></div><p>Hi Brian:<br> 1) Indeed the data D does not need to be Gaussian for the theory to hold, I should probably have made that more clear in the article. However, talking about covariance matrices often does not have much meaning in highly non-Gaussian data.</p><p>2) That depends on whether D is a row vector or a column vector I suppose. In this case, if each column of D is a data entry, then R*D = (D^t*R)^t</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=49#respond" onclick="return addComment.moveForm(&quot;div-comment-49&quot;, &quot;49&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment odd alt thread-odd thread-alt depth-1 parent" id="comment-88"><div id="div-comment-88" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Konstantin</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-88"> August 18, 2014 at 1:47 am</a></div><p>Thank you for this great post! But let me please correct one fundamental mistake that you made. The square root of covariance matrix M is not equal to R * S. The square root of M equals R * S * R’, where R’ is transposed R. Proof: (R * S * R’) * (R * S * R’) = R * S * R’ * R * S * R’ = R * S * S * R’ = T * T’ = M. And, of course, T is not a symmetric matrix (in your post T = T’, which is wrong).</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=88#respond" onclick="return addComment.moveForm(&quot;div-comment-88&quot;, &quot;88&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment byuser comment-author-esurior bypostauthor even depth-2" id="comment-89"><div id="div-comment-89" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://www.visiondummy.com/" rel="external nofollow" class="url">Vincent Spruyt</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-89"> August 18, 2014 at 8:29 am</a></div><p>Thanks a lot for noticing! You are right indeed, I will get back about this soon (don’t really have time right now).</p><p>Edit: I just fixed this mistake. Sorry for the long delay, I didn’t find the time before. Thanks a lot for your feedback!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=89#respond" onclick="return addComment.moveForm(&quot;div-comment-89&quot;, &quot;89&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment odd alt thread-even depth-1 parent" id="comment-97"><div id="div-comment-97" class="comment-body"><div class="comment-author vcard"> <cite class="fn">srinivas kumar</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-97"> October 29, 2014 at 2:52 pm</a></div><p>Very Useful Article <img src="./A geometric interpretation of the covariance matrix_files/icon_smile.gif" alt=":)" class="wp-smiley"> What I feel needs to be included is the interpretation of the action of the covariance matrix as a linear operator. For example, the eigen vectors of the covariance matrix form the principal components in PCA. So, basically , the covariance matrix takes an input data point ( vector ) and if it resembles the data points from which the operator was obtained, it keeps it invariant ( upto scaling ). Is there a better way to interpret the eigenvectors of covariance matrix ?</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=97#respond" onclick="return addComment.moveForm(&quot;div-comment-97&quot;, &quot;97&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment byuser comment-author-esurior bypostauthor even depth-2" id="comment-129"><div id="div-comment-129" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://www.visiondummy.com/" rel="external nofollow" class="url">Vincent Spruyt</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-129"> March 7, 2015 at 2:18 pm</a></div><p>Hi Kumar, great point! This is basically captured by equations 13 and 14, but I just added a short section to make this a bit more clear in the article.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=129#respond" onclick="return addComment.moveForm(&quot;div-comment-129&quot;, &quot;129&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-142"><div id="div-comment-142" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Srinivas Kumar</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-142"> March 7, 2015 at 3:02 pm</a></div><p>“..the eigenvectors represent the directions of the largest variance of the data, the eigenvalues represent the magnitude of this variance in those directions..” … <img src="./A geometric interpretation of the covariance matrix_files/icon_smile.gif" alt=":-)" class="wp-smiley"> Thanks a lot for expressing it so precisely.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=142#respond" onclick="return addComment.moveForm(&quot;div-comment-142&quot;, &quot;142&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1 parent" id="comment-159"><div id="div-comment-159" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Paul</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-159"> April 25, 2015 at 5:40 pm</a></div><p>Thank you for such an intuitive article.  I have spent countless hours over countless days trying to picture exactly what you described.  I wonder if you can clarify something in the writing, though.  When you first talk about vector v, throughout the entire paragraph, it is referred to both as a unit vector and a vector whose length is set to match the spread of data in the direction of v.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=159#respond" onclick="return addComment.moveForm(&quot;div-comment-159&quot;, &quot;159&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment odd alt depth-2" id="comment-160"><div id="div-comment-160" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Paul</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-160"> April 25, 2015 at 5:53 pm</a></div><p>By the way, would you know of a similarly intuitive description of cov(X,Y), where X and Y are disjoint sets of random variables?</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=160#respond" onclick="return addComment.moveForm(&quot;div-comment-160&quot;, &quot;160&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment even thread-odd thread-alt depth-1 parent" id="comment-161"><div id="div-comment-161" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Manoj Nambiar</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-161"> May 14, 2015 at 5:30 am</a></div><p>Quoting a portion of the text above ” …..we should choose its components such that the covariance matrix \vec{v}^{\intercal} \Sigma \vec{v} of the projected data is as large as possible….”</p><p>That quantity “\vec{v}^{\intercal} \Sigma \vec{v}”  (sorry – I am not able to do a graphical paste – but I hope you know what I mean) is not a matrix – It is a scalar quantity – isn’t it?</p><p>Or what you wanted to say was ” …..we should choose its components such that the covariance of the data with the vector v is as large as possible….”. And this covariance is a term of the Raliegh’s coefficient ……</p><p>May be there is a better way to put …</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=161#respond" onclick="return addComment.moveForm(&quot;div-comment-161&quot;, &quot;161&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment odd alt depth-2" id="comment-342"><div id="div-comment-342" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://harmyder.wordpress.com/" rel="external nofollow" class="url">harmyder</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-342"> January 17, 2017 at 9:30 am</a></div><p>The better way is to say that it is just a variance of projected data. So, that is a mistake, it should be variance, not covariance.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=342#respond" onclick="return addComment.moveForm(&quot;div-comment-342&quot;, &quot;342&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment even thread-even depth-1" id="comment-165"><div id="div-comment-165" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="https://lukasbrausch.wordpress.com/" rel="external nofollow" class="url">Lukas</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-165"> June 2, 2015 at 5:09 pm</a></div><p>Hello Vincent,</p><p>Thank you very much for this blog post. I have one question though concerning figure 4: Shouldn’t the magenta eigenvector in the right part of the picture point downwards? Otherwise, we wouldn’t have a proper rotation.</p><p>Best regards,<br> Lukas</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=165#respond" onclick="return addComment.moveForm(&quot;div-comment-165&quot;, &quot;165&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-166"><div id="div-comment-166" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Gordon Marney</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-166"> June 8, 2015 at 12:59 am</a></div><p>The correlations you showin figure 5 look a lot like Reduced Major Axis.  How do do the eigenvectors and RMA compare?.  Of course, there is nothing like eigenvalues in the RMA but could they be estimated from the ranges of values after rotation of the RMA regression?</p><p>Gordon</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=166#respond" onclick="return addComment.moveForm(&quot;div-comment-166&quot;, &quot;166&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-167"><div id="div-comment-167" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="https://www.facebook.com/app_scoped_user_id/10152979046415954/" rel="external nofollow" class="url">Inderjit Nanda</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-167"> June 9, 2015 at 12:49 pm</a></div><p>I love to reread your articles. Hope to see more such intuitive topics!!!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=167#respond" onclick="return addComment.moveForm(&quot;div-comment-167&quot;, &quot;167&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-168"><div id="div-comment-168" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Mehdi Pedram</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-168"> June 12, 2015 at 4:24 pm</a></div><p>Great Thank you.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=168#respond" onclick="return addComment.moveForm(&quot;div-comment-168&quot;, &quot;168&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-171"><div id="div-comment-171" class="comment-body"><div class="comment-author vcard"> <cite class="fn">seravee</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-171"> June 17, 2015 at 9:09 am</a></div><p>Thanks a lot. Very intuitive articles on the covariance matrix.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=171#respond" onclick="return addComment.moveForm(&quot;div-comment-171&quot;, &quot;171&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-172"><div id="div-comment-172" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Pradeep</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-172"> June 21, 2015 at 10:31 am</a></div><p>Always wondered why Eigen vectors of covariance matrix and the actual data were similar. Thanks for the tutorial. It helped in clearing the doubt.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=172#respond" onclick="return addComment.moveForm(&quot;div-comment-172&quot;, &quot;172&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-201"><div id="div-comment-201" class="comment-body"><div class="comment-author vcard"> <cite class="fn">haining</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-201"> September 11, 2015 at 6:36 pm</a></div><p>Great article!!! It’s soooooo helpful, thank you <img src="./A geometric interpretation of the covariance matrix_files/icon_smile.gif" alt=":)" class="wp-smiley"></p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=201#respond" onclick="return addComment.moveForm(&quot;div-comment-201&quot;, &quot;201&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-203"><div id="div-comment-203" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Priyamvad</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-203"> September 13, 2015 at 11:52 pm</a></div><p>Thanks for sharing this article, it’s a wonderful read!<br> Am I correct in understanding that the transformation TT^t for the covariance matrix will apply when transforming any data by T, not just for white data?</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=203#respond" onclick="return addComment.moveForm(&quot;div-comment-203&quot;, &quot;203&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-212"><div id="div-comment-212" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Joe</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-212"> October 30, 2015 at 2:21 pm</a></div><p>Nice article. Thank you so much!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=212#respond" onclick="return addComment.moveForm(&quot;div-comment-212&quot;, &quot;212&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-222"><div id="div-comment-222" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://www.whoi.edu/sbl/liteSite.do?litesiteid=8232" rel="external nofollow" class="url">Jim Price</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-222"> December 1, 2015 at 4:03 pm</a></div><p>Superb!</p><p>It is just awesome that you are so open to suggestions and then make the changes for the benefit of all of us.</p><p>Thank You, Thank You, Thank You!!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=222#respond" onclick="return addComment.moveForm(&quot;div-comment-222&quot;, &quot;222&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-242"><div id="div-comment-242" class="comment-body"><div class="comment-author vcard"> <cite class="fn">dk sunil</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-242"> January 18, 2016 at 9:59 pm</a></div><p>excellent article. gave me a whole new perspective of covariance matrix.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=242#respond" onclick="return addComment.moveForm(&quot;div-comment-242&quot;, &quot;242&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1 parent" id="comment-244"><div id="div-comment-244" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Gennaro</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-244"> January 22, 2016 at 11:53 am</a></div><p>Thanks man, great article! So useful for my PhD !<br> Do you know of any mathematics book where I can find a rigorous dissertation about this? My professor wants me to be as meticulous as possible <img src="./A geometric interpretation of the covariance matrix_files/icon_sad.gif" alt=":(" class="wp-smiley"><br> Thank you in advance!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=244#respond" onclick="return addComment.moveForm(&quot;div-comment-244&quot;, &quot;244&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment even depth-2" id="comment-355"><div id="div-comment-355" class="comment-body"><div class="comment-author vcard"> <cite class="fn">simon</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-355"> March 10, 2017 at 6:11 am</a></div><p>Do you know of any mathematics book where I can find a rigorous dissertation about this?</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=355#respond" onclick="return addComment.moveForm(&quot;div-comment-355&quot;, &quot;355&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment odd alt thread-even depth-1" id="comment-250"><div id="div-comment-250" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Nariman</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-250"> February 12, 2016 at 2:51 pm</a></div><p>education system need people like you</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=250#respond" onclick="return addComment.moveForm(&quot;div-comment-250&quot;, &quot;250&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-odd thread-alt depth-1" id="comment-258"><div id="div-comment-258" class="comment-body"><div class="comment-author vcard"> <cite class="fn">beena</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-258"> March 20, 2016 at 6:22 pm</a></div><p>it was really an informative article,thanks a lot</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=258#respond" onclick="return addComment.moveForm(&quot;div-comment-258&quot;, &quot;258&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-even depth-1" id="comment-259"><div id="div-comment-259" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Marcell</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-259"> March 29, 2016 at 6:56 pm</a></div><p>This is a great article, thank you so much! I completely agree with your motivation to write things down in a simple way, rather than trying to sound smart to people who already know everything. I learned so much from your blog in a short time.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=259#respond" onclick="return addComment.moveForm(&quot;div-comment-259&quot;, &quot;259&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-odd thread-alt depth-1" id="comment-263"><div id="div-comment-263" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Treble</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-263"> April 15, 2016 at 2:16 am</a></div><p>Thanks for this!  This and the companion eigen decomposition article were exactly what I was looking for, and much easier to understand than other resources I found.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=263#respond" onclick="return addComment.moveForm(&quot;div-comment-263&quot;, &quot;263&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-even depth-1" id="comment-265"><div id="div-comment-265" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Artur RC</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-265"> April 18, 2016 at 6:23 pm</a></div><p>Great text. Good job! Thank you <img src="./A geometric interpretation of the covariance matrix_files/icon_smile.gif" alt=":)" class="wp-smiley"></p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=265#respond" onclick="return addComment.moveForm(&quot;div-comment-265&quot;, &quot;265&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-odd thread-alt depth-1" id="comment-266"><div id="div-comment-266" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Shibumon Alampatta</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-266"> April 21, 2016 at 5:56 am</a></div><p>Great article, but one doubt. It was mentioned that direction of eigen vector remains unchanged when linear transformation is applied. But in Fig. 10, direction of the vector is also changed. Can you please explain it?</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=266#respond" onclick="return addComment.moveForm(&quot;div-comment-266&quot;, &quot;266&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-even depth-1 parent" id="comment-269"><div id="div-comment-269" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://www.golum.com/" rel="external nofollow" class="url">ninjajack</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-269"> April 28, 2016 at 7:20 am</a></div><p>Very useful. Thanks!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=269#respond" onclick="return addComment.moveForm(&quot;div-comment-269&quot;, &quot;269&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div><ul class="children"><li class="comment even depth-2" id="comment-351"><div id="div-comment-351" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Manoj Gupta</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-351"> March 6, 2017 at 10:41 am</a></div><p>Very good explain and worthful.<br> But I have doubt why does eigenvector have one direction even though spread is in both directions.<br> It’s true that both cancel out and we are left with zero…<br> Where I am going geometrically worng.<br> Second<br> Why don’t we a complex eigen vector conjugate when we rotate the white data by rotational matrix….</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=351#respond" onclick="return addComment.moveForm(&quot;div-comment-351&quot;, &quot;351&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ul></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-295"><div id="div-comment-295" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Summer</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-295"> July 28, 2016 at 7:51 pm</a></div><p>Great post! I have one question though. What if some of the eigenvalues are negative? Then how can you decompose L into SS^T</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=295#respond" onclick="return addComment.moveForm(&quot;div-comment-295&quot;, &quot;295&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-308"><div id="div-comment-308" class="comment-body"><div class="comment-author vcard"> <cite class="fn">jiissee bixee</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-308"> September 18, 2016 at 6:56 pm</a></div><p>In figure 4. Did you swap the element in the vector<br> in the Left    : I think it should be V1 = [1 0] and V2 = [0 1]<br> in the Right :  I think it should be V1 = [0 1] and V2 = [1 0]<br> if I missunderstand something  im sorry</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=308#respond" onclick="return addComment.moveForm(&quot;div-comment-308&quot;, &quot;308&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-311"><div id="div-comment-311" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Niranjan Kotha</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-311"> September 27, 2016 at 12:34 am</a></div><p>Very useful article but I faced some errors while implementing this.<br> I tried to do svd decomposition of the covariance matrix and got L matrix as the square of scaling coefficients(not exactly equal but very close Note:implemented in matlab) but the the Rotation matrix I got  weird matrix where the first element in the matrix cos(theta) is negative and last element in the matrix is postive. Here is the code if you want to have a look (<a href="https://drive.google.com/open?id=0B0Dif3DoeegwY1NuNlFUVUc4eXFsTGtSeFl4YkFDMXRDWHVj" onclick="__gaTracker(&#39;send&#39;, &#39;event&#39;, &#39;outbound-comment&#39;, &#39;https://drive.google.com/open?id=0B0Dif3DoeegwY1NuNlFUVUc4eXFsTGtSeFl4YkFDMXRDWHVj&#39;, &#39;https://drive.google.com/open?id=0B0Dif3DoeegwY1NuNlFUVUc4eXFsTGtSeFl4YkFDMXRDWHVj&#39;);" rel="nofollow">https://drive.google.com/open?id=0B0Dif3DoeegwY1NuNlFUVUc4eXFsTGtSeFl4YkFDMXRDWHVj</a>)</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=311#respond" onclick="return addComment.moveForm(&quot;div-comment-311&quot;, &quot;311&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-315"><div id="div-comment-315" class="comment-body"><div class="comment-author vcard"> <cite class="fn">J. tipton</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-315"> October 1, 2016 at 6:28 pm</a></div><p>Very lucid article. I have a question. If one of the eigenvalues (say lambda) has a multiplicity bigger than 1 (say 2 for simplicity), then, theoretically, one can chose different sets of  two  eigenvectors associated with lambda. How does that Does that affect the interpretation? This is relevant when one is constructing the principal components that would give most information about the data. Thank for your attention to me question. –ekwaysan</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=315#respond" onclick="return addComment.moveForm(&quot;div-comment-315&quot;, &quot;315&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-317"><div id="div-comment-317" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Duong Tuan Nguyen</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-317"> October 3, 2016 at 4:36 am</a></div><p>Very useful article. However, I don’t understand how \vec{v}^{\intercal} \Sigma \vec{v} is the variance of the projected data. Can anyone explain it for me?</p><p>Thanks in advance!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=317#respond" onclick="return addComment.moveForm(&quot;div-comment-317&quot;, &quot;317&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-325"><div id="div-comment-325" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://humansofportsmouth.wordpress.com/" rel="external nofollow" class="url">humansofportsmouth</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-325"> November 13, 2016 at 4:29 pm</a></div><p>Thank you a lot.. I was trying to implement my mcmc code using a proposal covariance matrix and thanks to your method everything is clear to me now <img src="./A geometric interpretation of the covariance matrix_files/icon_smile.gif" alt=":)" class="wp-smiley"></p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=325#respond" onclick="return addComment.moveForm(&quot;div-comment-325&quot;, &quot;325&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-330"><div id="div-comment-330" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Dave</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-330"> November 25, 2016 at 8:42 pm</a></div><p>Awesome article! Really helped me to understand this eigenvalue/eigenvector stuff :)..thanks!!!</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=330#respond" onclick="return addComment.moveForm(&quot;div-comment-330&quot;, &quot;330&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment even thread-even depth-1" id="comment-366"><div id="div-comment-366" class="comment-body"><div class="comment-author vcard"> <cite class="fn"><a href="http://renzocoppola.wordpress.com/" rel="external nofollow" class="url">renzocoppola</a></cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-366"> April 2, 2017 at 8:09 am</a></div><p>Really cool. I though I would never find the correlation of these matrices and transformations in the CMA-ES algorithm.</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=366#respond" onclick="return addComment.moveForm(&quot;div-comment-366&quot;, &quot;366&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li><li class="comment odd alt thread-odd thread-alt depth-1" id="comment-374"><div id="div-comment-374" class="comment-body"><div class="comment-author vcard"> <cite class="fn">Kevin</cite> <span class="says">says:</span></div><div class="comment-meta commentmetadata"><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#comment-374"> April 24, 2017 at 12:27 am</a></div><p>very instructive!! thank you-</p><div class="reply"> <a class="comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/?replytocom=374#respond" onclick="return addComment.moveForm(&quot;div-comment-374&quot;, &quot;374&quot;, &quot;respond&quot;, &quot;440&quot;)">Reply</a></div></div></li></ol><div class="navigation"><div class="alignleft"></div><div class="alignright"></div></div><div id="respond" class="comment-respond"><h3 id="reply-title" class="comment-reply-title">Comments are very welcome! <small><a rel="nofollow" id="cancel-comment-reply-link" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#respond" style="display:none;">Cancel reply</a></small></h3><div id="commentform" class="comment-form"> <iframe src="./A geometric interpretation of the covariance matrix_files/saved_resource.html" allowtransparency="false" style="width:100%; height: 430px;border:0px;" frameborder="0" scrolling="no" name="jetpack_remote_comment" id="jetpack_remote_comment"></iframe></div></div> <input type="hidden" name="comment_parent" id="comment_parent" value=""></div></div><div id="footerads"></div></div><div id="sidebar"><div class="box clearfloat"><div class="boxinside clearfloat"><h4 class="widgettitle">Subscribe to this blog!</h4><div class="textwidget"><div id="optinforms-form5-container"><form method="post" target="_blank" action="http://visiondummy.us10.list-manage.com/subscribe/post?u=c435905e10ead915f3917d694&amp;id=bbdfb33a9f"><div id="optinforms-form5" style="background:#ffffff;"><div id="optinforms-form5-container-left"><div id="optinforms-form5-title" style="font-family:News Cycle; font-size:24px; line-height:24px; color:#fd4326">JOIN MY NEWSLETTER</div><input type="text" id="optinforms-form5-name-field" name="FNAME" placeholder="Enter Your Name" style="font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000000"><input type="text" id="optinforms-form5-email-field" name="EMAIL" placeholder="Enter Your Email" style="font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000000"><input type="submit" name="submit" id="optinforms-form5-button" value="SUBSCRIBE" style="font-family:Arial, Helvetica, sans-serif; font-size:16px; color:#FFFFFF; background-color:#fd4326"></div><div id="optinforms-form5-container-right"><div id="optinforms-form5-subtitle" style="font-family:Georgia; font-size:16px; color:#444444">Receive my newsletter to get notified when new articles and code snippets become available on my blog!</div><div id="optinforms-form5-disclaimer" style="font-family:Georgia, Times New Roman, Times, serif; font-size:14px; color:#727272">I hate spam. Your email address will not be sold or shared with anyone else.</div></div><div class="clear"></div></div><div class="clear"></div></form></div><div class="clear"></div></div></div></div><div class="box clearfloat"><div class="boxinside clearfloat"><h4 class="widgettitle">Follow on Twitter</h4><div class="textwidget"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" title="Twitter Follow Button" src="./A geometric interpretation of the covariance matrix_files/follow_button.7bf2b0e802ada47dae9548b7a1739fed.en.html" style="position: static; visibility: visible; width: 175px; height: 28px;" data-screen-name="vincent_spruyt"></iframe><script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document,'script','twitter-wjs');</script></div></div></div><div class="box clearfloat"><div class="boxinside clearfloat"><h4 class="widgettitle">Article topics</h4><ul><li class="cat-item cat-item-3"><a href="http://www.visiondummy.com/category/dimensionality-reduction/" title="View all posts filed under Dimensionality reduction">Dimensionality reduction</a> (2)<ul class="children"><li class="cat-item cat-item-33"><a href="http://www.visiondummy.com/category/dimensionality-reduction/feature-extraction/" title="View all posts filed under Feature extraction">Feature extraction</a> (2)</li></ul></li><li class="cat-item cat-item-17"><a href="http://www.visiondummy.com/category/math-basics/" title="View all posts filed under Math basics">Math basics</a> (4)<ul class="children"><li class="cat-item cat-item-18"><a href="http://www.visiondummy.com/category/math-basics/linear-algebra/" title="View all posts filed under Linear algebra">Linear algebra</a> (2)</li><li class="cat-item cat-item-22"><a href="http://www.visiondummy.com/category/math-basics/statistics/" title="View all posts filed under Statistics">Statistics</a> (2)</li></ul></li><li class="cat-item cat-item-47"><a href="http://www.visiondummy.com/category/other/" title="View all posts filed under Other">Other</a> (1)</li></ul></div></div><div class="box clearfloat"><div class="boxinside clearfloat"><h4 class="widgettitle">Recent Posts</h4><ul><li> <a href="http://www.visiondummy.com/2017/04/deep-learning-for-long-term-prediction/">Deep learning for long-term predictions</a></li><li> <a href="http://www.visiondummy.com/2014/05/feature-extraction-using-pca/">Feature extraction using PCA</a></li><li> <a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/">A geometric interpretation of the covariance matrix</a></li><li> <a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">The Curse of Dimensionality in classification</a></li><li> <a href="http://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/">How to draw a covariance error ellipse?</a></li></ul></div></div><div class="box clearfloat"><div class="boxinside clearfloat"><h4 class="widgettitle">Rss feed</h4><p><a href="http://www.visiondummy.com/feed/" title="Subscribe to Posts"><img src="./A geometric interpretation of the covariance matrix_files/orange-medium.png" alt="RSS Feed"></a>&nbsp;<a href="http://www.visiondummy.com/feed/" title="Subscribe to Posts">RSS - Posts</a></p><p><a href="http://www.visiondummy.com/comments/feed/" title="Subscribe to Comments"><img src="./A geometric interpretation of the covariance matrix_files/orange-medium.png" alt="RSS Feed"></a>&nbsp;<a href="http://www.visiondummy.com/comments/feed/" title="Subscribe to Comments">RSS - Comments</a></p></div></div></div></div></div><div id="bottom-menu"><div id="bottom-menu-inner" class="clearfix"><div id="bottom-menu-1"></div><div id="bottom-menu-2"></div><div id="bottom-menu-3"></div><div id="bottom-menu-4"></div></div></div><div id="footer"><div id="footer-inner" class="clearfix"> <a href="http://www.visiondummy.com/" title="Computer vision for dummies">Computer vision for dummies</a> © 2014<a class="backtop" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#top">↑</a></div></div></div><div class="popupally-configuration" style="display:none;">{"enable-exit-intent-popup":"true","cookie-duration":14,"popup-selector":"#popup-box-sxzw-1","popup-class":"popupally-opened-sxzw-1","cookie-name":"popupally-cookie-1","close-trigger":".popup-click-close-trigger-1"}</div><div id="popup-box-sxzw-1" class="popupally-overlay-sxzw-1 popup-click-close-trigger-1"><div class="popupally-outer-sxzw-1"><div class="popupally-inner-sxzw-1"><div class="popupally-center-sxzw"><div class="desc-sxzw">Wait! Forgetting something?</div><div class="logo-row-sxzw"><div class="clear-sxzw"></div> <img class="logo-img-sxzw" src="./A geometric interpretation of the covariance matrix_files/robot2.png" alt=""><div class="logo-text-sxzw">Subscribe to my newsletter to get notified when new articles and code samples become available!</div><div class="clear-sxzw"></div></div><form action="http://visiondummy.us10.list-manage.com/subscribe/post?u=c435905e10ead915f3917d694&amp;id=bbdfb33a9f" class="content-sxzw" method="post"><input type="text" name="FNAME" class="field-sxzw" placeholder="Enter your first name here"> <input type="text" name="EMAIL" class="field-sxzw" placeholder="Enter a valid email here"> <input type="submit" class="submit-sxzw" value="Subscribe"></form><div class="privacy-sxzw">Your information will *never* be shared or sold to a 3rd party.</div></div></div><div title="Close" class="popupally-close-sxzw popup-click-close-trigger-1"></div></div></div><script type="text/javascript">WPCOM_sharing_counts={"http:\/\/www.visiondummy.com\/2014\/04\/geometric-interpretation-covariance-matrix\/":440}</script><script type="text/javascript">jQuery(document).on('ready post-load',function(){jQuery('a.share-facebook').on('click',function(){window.open(jQuery(this).attr('href'),'wpcomfacebook','menubar=1,resizable=1,width=600,height=400');return false;});});</script><script type="text/javascript">jQuery(document).on('ready post-load',function(){jQuery('a.share-google-plus-1').on('click',function(){window.open(jQuery(this).attr('href'),'wpcomgoogle-plus-1','menubar=1,resizable=1,width=480,height=550');return false;});});</script><script type="text/javascript">jQuery(document).on('ready post-load',function(){jQuery('a.share-twitter').on('click',function(){window.open(jQuery(this).attr('href'),'wpcomtwitter','menubar=1,resizable=1,width=600,height=350');return false;});});</script><script type="text/javascript">jQuery(document).on('ready post-load',function(){jQuery('a.share-linkedin').on('click',function(){window.open(jQuery(this).attr('href'),'wpcomlinkedin','menubar=1,resizable=1,width=580,height=450');return false;});});</script><script type="text/javascript">var tocplus={"smooth_scroll":"1","visibility_show":"show","visibility_hide":"hide","width":"Auto"};</script><script type="text/javascript" src="./A geometric interpretation of the covariance matrix_files/front.min.js"></script><script type="text/javascript" src="./A geometric interpretation of the covariance matrix_files/devicepx-jetpack.js"></script><script type="text/javascript">var recaptcha_options={"lang":"en"};</script><script type="text/javascript" src="./A geometric interpretation of the covariance matrix_files/sharing.js"></script><!--[if IE]><script type="text/javascript">if(0===window.location.hash.indexOf('#comment-')){window.location.hash=window.location.hash;}</script><![endif]--><script type="text/javascript">/*<![CDATA[*/var comm_par_el=document.getElementById('comment_parent'),comm_par=(comm_par_el&&comm_par_el.value)?comm_par_el.value:'',frame=document.getElementById('jetpack_remote_comment'),tellFrameNewParent;tellFrameNewParent=function(){if(comm_par){frame.src="http://jetpack.wordpress.com/jetpack-comment/?blogid=65128443&postid=440&comment_registration=0&require_name_email=1&stc_enabled=0&stb_enabled=0&show_avatars=0&avatar_default=mystery&greeting=Comments+are+very+welcome%21&greeting_reply=Leave+a+Reply+to+%25s&color_scheme=light&lang=en-US&jetpack_version=3.1.1&sig=986e335adc5676076e7e572182b50ba8871fb8a0#parent=http%3A%2F%2Fwww.visiondummy.com%2F2014%2F04%2Fgeometric-interpretation-covariance-matrix%2F"+'&replytocom='+parseInt(comm_par,10).toString();}else{frame.src="http://jetpack.wordpress.com/jetpack-comment/?blogid=65128443&postid=440&comment_registration=0&require_name_email=1&stc_enabled=0&stb_enabled=0&show_avatars=0&avatar_default=mystery&greeting=Comments+are+very+welcome%21&greeting_reply=Leave+a+Reply+to+%25s&color_scheme=light&lang=en-US&jetpack_version=3.1.1&sig=986e335adc5676076e7e572182b50ba8871fb8a0#parent=http%3A%2F%2Fwww.visiondummy.com%2F2014%2F04%2Fgeometric-interpretation-covariance-matrix%2F";}};if('undefined'!==typeof addComment){addComment._Jetpack_moveForm=addComment.moveForm;addComment.moveForm=function(commId,parentId,respondId,postId){var returnValue=addComment._Jetpack_moveForm(commId,parentId,respondId,postId),cancelClick,cancel;if(false===returnValue){cancel=document.getElementById('cancel-comment-reply-link');cancelClick=cancel.onclick;cancel.onclick=function(){var cancelReturn=cancelClick.call(this);if(false!==cancelReturn){return cancelReturn;}
if(!comm_par){return cancelReturn;}
comm_par=0;tellFrameNewParent();return cancelReturn;};}
if(comm_par==parentId){return returnValue;}
comm_par=parentId;tellFrameNewParent();return returnValue;};}
if(window.postMessage){if(document.addEventListener){window.addEventListener('message',function(event){if("http:\/\/jetpack.wordpress.com"!==event.origin){return;}
jQuery(frame).height(event.data);});}else if(document.attachEvent){window.attachEvent('message',function(event){if("http:\/\/jetpack.wordpress.com"!==event.origin){return;}
jQuery(frame).height(event.data);});}}/*]]>*/</script><script src="./A geometric interpretation of the covariance matrix_files/e-201723.js" type="text/javascript"></script><script type="text/javascript">st_go({v:'ext',j:'1:3.1.1',blog:'65128443',post:'440',tz:'1'});var load_cmc=function(){linktracker_init(65128443,440,2);};if(typeof addLoadEvent!='undefined')addLoadEvent(load_cmc);else load_cmc();</script><img src="./A geometric interpretation of the covariance matrix_files/g.gif" alt=":)" width="6" height="5" id="wpstats"><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe" src="./A geometric interpretation of the covariance matrix_files/saved_resource(3).html"></iframe></body></html>